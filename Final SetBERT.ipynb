{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcee43e-2896-4be5-b036-fcc31aba19a8",
   "metadata": {},
   "source": [
    "# SetBERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551489c8-985e-42f5-bbba-4bcfcc87cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "import sys\n",
    "sys.path.append(\"../deep-learning-dna\")\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "import wandb\n",
    "from lmdbm import Lmdb\n",
    "import settransformer as st\n",
    "from common.models import CustomModel\n",
    "from common.models import dnabert\n",
    "import os, glob\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe77443-f999-448b-90fe-894087eaaf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to WandB\n",
    "run = wandb.init(entity=\"cguptil\", project=\"SetBERT\", name=\"SetBERT_Run_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317f600-141e-4c32-8abb-0e1a6921f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategy = tfu.strategy.gpu(0) #Optional strategy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144dd6ac-cf8d-4dc9-984c-3226d1a0be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8d6dc-2c57-41fb-86b4-defaca0f88ad",
   "metadata": {},
   "source": [
    "## Custom classes for Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e36d3-e711-4dbc-9379-8ae277171afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted mask from David Ludwig's DNABERT architecture, only Inverted Mask is used here\n",
    "class InvertMask(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Invert the current mask. Useful for DNABERT models where we *want* to pay attention to the\n",
    "    masked elements.\n",
    "    \"\"\"\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return tf.logical_not(mask)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # tf.print(inputs)\n",
    "        # tf.print(inputs + 0)\n",
    "        return inputs + 0 # hacky, but without modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9915e03-109b-4264-ad0d-6ae35c9e8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Set Mask implementation. Blocks out 15% of sequences in a batch. Does not block individual embedding values\n",
    "#in sequences, but rather the entire sequence. Used for BERT style training.\n",
    "class SetMask(keras.layers.Layer):\n",
    "    def __init__(self, mask_ratio, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mask_ratio = tf.Variable(mask_ratio, trainable=False, dtype=tf.float32, name='Mask_Ratio')\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        mask = self.compute_mask(inputs, mask)\n",
    "        return tf.cast(mask, dtype=tf.float32) * inputs\n",
    "    \n",
    "    def compute_mask(self, inputs, mask):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        embed_dim = tf.shape(inputs)[2]\n",
    "        mask_len = tf.cast(tf.cast(seq_len, dtype=tf.float32) * self.mask_ratio, dtype=tf.int32)\n",
    "        \n",
    "        random = tf.random.uniform((batch_size, seq_len), 0, 1)\n",
    "        values, indices = tf.math.top_k(random, mask_len)\n",
    "        batch_indices = tf.reshape(tf.repeat(tf.range(batch_size), mask_len), (-1,1))\n",
    "        embedding_indices = tf.reshape(indices, (-1, 1))\n",
    "        indices = tf.concat((batch_indices, embedding_indices), axis=1)\n",
    "        mask = tf.ones((batch_size, seq_len))\n",
    "        mask = tf.tensor_scatter_nd_update(mask, indices, tf.zeros((batch_size * mask_len)))\n",
    "        mask = tf.tile(tf.expand_dims(mask, axis=2), (1, 1, embed_dim))\n",
    "        return tf.cast(mask, dtype=tf.bool)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"mask_ratio\": self.mask_ratio.numpy()\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb871a54-3c8b-46ac-9188-467eecce67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appends a class token to the beginning of each set with the same dimensionality as sequences.\n",
    "#Used for BERT style training.\n",
    "class SetClassToken(keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.class_token = self.add_weight(shape=(1, 1, embedding_dim), initializer='random_normal',\n",
    "                                           trainable=True, name='Class_token')\n",
    "    \n",
    "    def compute_mask(self, inputs, mask):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        token_mask = tf.ones((batch_size, 1, self.embedding_dim), dtype=tf.bool)\n",
    "        return tf.concat((token_mask, mask), axis=1)\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        tokens = tf.tile(self.class_token, (batch_size, 1, 1))\n",
    "        return tf.concat((tokens, inputs), axis=1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embedding_dim\": self.embedding_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85973364-655d-47a8-a0ea-f1c196d1f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allows for subbatch usage to have larger batches without running out of memory\n",
    "class SubBatchModel(CustomModel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        return self.model(inputs, training=training)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"model\": self.model\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535feba-39ad-4377-9687-a74597e44ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generator used for shuffling training and validation data.\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, samples, batch_size, subsample_size, num_batches, rng=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.subsample_size = subsample_size\n",
    "        self.samples = self.open_samples(samples)\n",
    "        self.num_batches = num_batches\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        batch = []\n",
    "        sample_indices = self.rng.integers(len(self.samples), size=self.batch_size)\n",
    "        for si in sample_indices:\n",
    "            indices = self.rng.choice(len(self.samples[si]), self.subsample_size, replace=False)\n",
    "            pre_embeddings = []\n",
    "            for i in indices:\n",
    "                pre_embeddings.append(np.asarray(np.frombuffer(self.samples[si][str(i)], dtype=np.float32)))\n",
    "            batch.append(pre_embeddings)\n",
    "            \n",
    "        return np.array(batch), np.array(batch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    \n",
    "    def open_samples(self, samples):\n",
    "        samples_final = []\n",
    "        for sample in samples:\n",
    "            store = Lmdb.open(sample, lock=False)\n",
    "            if len(store) < self.subsample_size:\n",
    "                print(f\"Warning: Sample '{sample}' only contains {len(store)} sequences. This sample will not be included.\")\n",
    "                store.close()\n",
    "                continue\n",
    "            samples_final.append(store)\n",
    "        return samples_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fd115-78e6-4b69-8114-d4eca9e5c7e7",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8924359-860f-4061-a592-27817ad9bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a4c16-b795-405c-b81b-45d864388f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Randomly shuffles pre-embedded sequence data in order to split the training and validation data randomly.\n",
    "#Essential for Monte-Carlo random cross validation.\n",
    "samples_train = []\n",
    "path = './pre_embedded_samples_complete/train'\n",
    "for filename in glob.glob(os.path.join(path, '*.db')):\n",
    "    samples_train.append(filename)\n",
    "random.shuffle(samples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c9758-b0f5-433e-be94-972250575e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(samples_train) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8369333-1465-48fb-a00d-732d219266db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples_train[:split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d09fc-6586-4785-a559-1630818952b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating seperate training and validation instances of data generators to keep data seperate.\n",
    "seq_gen_train = DataGenerator(samples_train[:split_index], 32, 1000, 20)\n",
    "seq_gen_val = DataGenerator(samples_train[split_index:], 32, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a1891-85ad-4727-91ba-796891355fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input((1000, 8)) #Takes in batches of 1000 sequences with 8 dimensional embeddings\n",
    "masking_layer = SetMask(mask_ratio=0.15) #Masking out 15% of sequences for BERT style training\n",
    "masked = masking_layer(input_layer)\n",
    "class_tokens = SetClassToken(8) #Adding 8-dim class token to each batch of 1000 sequences\n",
    "tokens_added = class_tokens(masked)\n",
    "for i in range(8): # Creating the 8 ISABs with 30 inducing points per block\n",
    "    ISAB = st.ISAB(8, 2, 30, pre_layernorm=True)\n",
    "    tokens_added = ISAB(tokens_added)\n",
    "inverted_mask = InvertMask()(tokens_added) #Inverted mask used to calculate loss on the 15% reconstruction\n",
    "output = keras.layers.Lambda(lambda x: x[:, 1:, :])(inverted_mask) #Removing the class token, returning 1000 sequences\n",
    "class_token_embeddings = keras.layers.Lambda(lambda x: x[:, 0, :])(inverted_mask) #Returning contextualized class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04409caf-329b-49e4-b067-74554b35905e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setbert = SubBatchModel(keras.Model(input_layer, output)) #create model instance which outputs 1000 sequences\n",
    "keras.utils.plot_model(setbert.model,show_shapes=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17682f48-f895-417e-a62b-bd9715d9e36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setbert_tokens = SubBatchModel(keras.Model(input_layer, class_token_embeddings)) #create model instance which outputs set class token\n",
    "keras.utils.plot_model(setbert_tokens.model,show_shapes=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf6c741-28ae-4915-b17a-c31a1ab5ed48",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50ce0c-873c-4730-9fb4-5529e0be401b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setbert.compile(optimizer=keras.optimizers.Nadam(1e-4), loss=tf.keras.losses.LogCosh()) #compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f1fd0-3acd-42a1-a681-1892755ce0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train model, save weights to WandB\n",
    "history = setbert.fit(seq_gen_train, epochs=300, subbatch_size=8, validation_data=seq_gen_val, callbacks=[wandb.keras.WandbCallback(save_weights_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8af4b-7606-4838-8f74-e4835e9e7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save backup weights to native system\n",
    "setbert.save_weights(\"./SetBERTSave\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ca588-092b-49bf-bb9f-dee0aa0b4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the validation samples to local machine\n",
    "with open(\"new_validation_samples.txt\", 'w') as f:\n",
    "    for sample in samples_train[split_index:]:\n",
    "        f.write(sample + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d59dde-a800-4df2-8eb2-95d63f59e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the validation samples to WandB\n",
    "wandb.save(\"new_validation_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8295e-5596-45d9-a813-5bba5cc506ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finishes WandB run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99185c14-53e2-43e9-8f4e-651df85e7736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot model loss and val loss\n",
    "plt.figure(1) \n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')  \n",
    "plt.xlabel('Epoch')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da89eb8-f639-4d72-93c9-6cc0ae3f65ff",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61e79d-91b4-446b-a5e4-01e72406721a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uses Validation data for testing\n",
    "samples_test = []\n",
    "for sample in samples_train[split_index:]:\n",
    "    samples_test.append(sample)\n",
    "samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c82cb7-851b-4708-b24b-76f7134a81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seperate data generator instances for each dataset as not to mix samples between sets.\n",
    "seq_gen_collection = []\n",
    "labels = []\n",
    "for sample in samples_test:\n",
    "    seq_gen = DataGenerator([sample], 32, 1000, 20)\n",
    "    if len(seq_gen.samples) > 0:\n",
    "        seq_gen_collection.append(seq_gen[0][0])\n",
    "        labels.append(sample)\n",
    "seq_gen_collection = seq_gen_collection[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb154b3a-2e04-4dd6-be2c-ab6ca4ae4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull off class tokens with newly trained model\n",
    "class_tokens = []\n",
    "for seq_gen in seq_gen_collection:\n",
    "    class_tokens.append(setbert_tokens.predict(seq_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae053f-51a4-4c50-95ad-ee1a84f47259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat class tokens together\n",
    "class_tokens = np.concatenate(class_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b9230-14af-4a92-b9bc-3dcb48925d8d",
   "metadata": {},
   "source": [
    "## MDS Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50679efb-f3f1-4197-8fd1-23c3185d52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db3c59-96c9-46f0-b507-e63dfabfc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distance matrix for distances between set embeddings captured by class tokens\n",
    "dist_mat = cdist(class_tokens, class_tokens)\n",
    "dist_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bce2d-028c-4c17-af92-d54744d4063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725e13d-9c90-458f-a4e4-263b0a635229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS plot with 8 components for the 8 dimensional embedding space\n",
    "mds = MDS(n_components=8, metric=True, dissimilarity='precomputed', n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2f079-e358-4682-aac3-9c3ab1ac9ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points = mds.fit_transform(dist_mat)\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a872d5-3028-4414-bee2-897b34e9aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plotting the first two principal components for visiualization\n",
    "for cluster in points[:,0:2].reshape(len(seq_gen_collection), 32, 2):\n",
    "    plt.scatter(*cluster.T)\n",
    "plt.legend([os.path.basename(s) for s in labels], loc='center left', bbox_to_anchor=(1., 0.5))\n",
    "plt.title(\"Bad Pre-Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e429b-2780-4668-a7c5-c48c8e7977a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17e89e-3708-49e2-a579-26150a307616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing chamfer distance between sets with David Ludwig's chamfer distance metric implementation.\n",
    "# Runs noticably longer than SetBERT distance matrix calculations.\n",
    "chamfer_dist = metrics.chamfer_distance_matrix(np.concatenate(seq_gen_collection), p=1, workers=10,\n",
    "                                fn=metrics.chamfer_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc235f9-d423-45e8-b93a-9d9d7f826de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chamfer_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd8bba-b60b-47c4-bf6d-75cd05b52007",
   "metadata": {},
   "outputs": [],
   "source": [
    "chamfer_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c53b3-efa0-4d27-ae9b-817350bfd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS plot with 8 components for the 8 dimensional embedding space\n",
    "mds_chamfer = MDS(n_components=8, metric=True, dissimilarity='precomputed', n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c7cca-5d22-4a6c-941d-869dfb6fbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "chamfer_points = mds_chamfer.fit_transform(chamfer_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba55304-4b70-4c70-9abc-42f9a736485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plotting the first two principal components for visiualization\n",
    "for cluster in chamfer_points[:, 0:2].reshape(len(seq_gen_collection), 32, 2):\n",
    "    plt.scatter(*cluster.T)\n",
    "plt.legend([os.path.basename(s) for s in labels], loc='center left', bbox_to_anchor=(1., 0.5))\n",
    "plt.title(\"Bad Pre-Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0dc4d-9db3-4f91-912e-f5df9e3f87e0",
   "metadata": {},
   "source": [
    "## KMeans Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8278b97-ba35-46e6-8b2e-4d5c94e734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c783b-1606-481d-865e-e2db3241c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans implemented on class tokens for comparison to Chamfer distance\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "labels = kmeans.fit_predict(class_tokens)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a7ec2-0cbb-47cc-b938-5dd7940483a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "chamfer_labels = kmeans.fit_predict(chamfer_points)\n",
    "chamfer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8d6c0-caf9-42d2-80ca-d43a0942346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b08dbc-9f9c-4a7f-8b0a-f50546be29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_labels:\n",
    "    plt.scatter(points[labels == i, 0], points[labels == i, 1], label = i)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1., 0.5))\n",
    "plt.title(\"KMeans SetBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(chamfer_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0623fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_labels:\n",
    "    plt.scatter(chamfer_points[chamfer_labels == i, 0], chamfer_points[chamfer_labels == i, 1], label = i)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1., 0.5))\n",
    "plt.title(\"KMeans Chamfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8838f8-d453-4f7c-8b80-5179c7e4d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b890b9-40cd-48bb-ba62-2770956e6ca0",
   "metadata": {},
   "source": [
    "## SetBERT Mutual Information Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134827c-a81a-4d97-816b-f152b6aaa1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mutual_info_score(np.repeat(np.arange(len(seq_gen_collection)), 32), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a880a8c-af25-4108-ae60-c2ef2835659d",
   "metadata": {},
   "source": [
    "## Chamfer Mutual Information Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8881bd2-f5d0-4867-9a6b-152262355b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mutual_info_score(np.repeat(np.arange(len(seq_gen_collection)), 32), chamfer_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
