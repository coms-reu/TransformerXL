{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccea5cd-89de-452b-afa6-703df9e9a604",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c14f8-8dd8-459f-81fe-a6708e6a8355",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {},
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb61447-8228-4b92-8f3f-0eddb8e33574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170304, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "data = []\n",
    "my_url = \"https://raw.githubusercontent.com/luisroque/deep-learning-articles/main/data/eng-por.txt\"\n",
    "with urllib.request.urlopen(my_url) as raw_data:\n",
    "    for line in raw_data:\n",
    "        data.append(line.decode(\"utf-8\").split('\\t')[0:2])\n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f65884-85ad-4ebd-ba51-29430504b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 170304//2\n",
    "split_point = 80\n",
    "data = data[100000:n_seq*2]\n",
    "np.random.shuffle(data) \n",
    "max_length = np.max([len(i) for i in data.flatten()]) + 2 \n",
    "max_length\n",
    "np.min([len(i) for i in data.flatten()]) + 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bbd23-6cbf-472f-bd6f-dc4a008b9072",
   "metadata": {},
   "source": [
    "---\n",
    "# Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c4c6ef-93df-4294-af8f-2fbaf01ca9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "    [mapping[i] for i in list(x)] + \\\n",
    "    [mapping['<STOP>']] + \\\n",
    "    [0]*(max_length-len(list(x))-2)\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b7c608-fe15-4420-a4fb-8356ebdcefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_eng = ['','<START>','<STOP>'] + list({char for word in data[:,0] for char in word})\n",
    "c_to_i_eng = {i_to_c_eng[i]:i for i in range(len(i_to_c_eng))}\n",
    "i_to_c_eng[1] = i_to_c_eng[2] = ''\n",
    "i_to_c_por = ['','<START>','<STOP>'] + list({char for word in data[:,1] for char in word})\n",
    "c_to_i_por = {i_to_c_por[i]:i for i in range(len(i_to_c_por))}\n",
    "i_to_c_por[1] = i_to_c_por[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcaf882a-b84a-49c5-8326-77f91be9d319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i_to_c_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6185f0ea-f949-427d-a6e4-d3c0ab45f960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i_to_c_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6889059-fdd9-43e2-b0bc-199ff07c4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 186)\n",
      "(80, 185)\n",
      "(80, 185)\n",
      "(70224, 186)\n",
      "(70224, 185)\n",
      "(70224, 185)\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_eng,max_length) for x in data[:,0]])\n",
    "Y = np.vstack([encode_seq(x,c_to_i_por,max_length) for x in data[:,1]])\n",
    "x_train = X[:split_point]\n",
    "x_test = X[split_point:]\n",
    "pre_y_train = Y[:,0:-1][:split_point]\n",
    "pre_y_test = Y[:,0:-1][split_point:]\n",
    "post_y_train = Y[:,1:][:split_point]\n",
    "post_y_test = Y[:,1:][split_point:]\n",
    "print(x_train.shape)\n",
    "print(pre_y_train.shape)\n",
    "print(post_y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(pre_y_test.shape)\n",
    "print(post_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1083fa6e-f561-4cad-8107-3b935126a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom só pensa em ganhar dinheiro.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(post_y_train[0], i_to_c_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca664bb9-544b-4264-8da4-34acaca2d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 186)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f725c81-51b2-4497-981a-a15981f9dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70224, 186)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6a03188-6146-4487-8483-b0a3296d4426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  89, 106,  63, 110,  87,  39, 110,  24,  47,  59,  87,  82,\n",
       "       110,  47,  63, 110,  49,  82,  59,  73,  82,   6, 110,  61,  33,\n",
       "        59,  73,  47,  33,   6, 106, 108,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154bc9ef-0e64-4ad5-96d4-13012994718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89, 106,  63, 110,  87,  39, 110,  24,  47,  59,  87,  82, 110,\n",
       "        47,  63, 110,  49,  82,  59,  73,  82,   6, 110,  61,  33,  59,\n",
       "        73,  47,  33,   6, 106, 108,   2,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e33b84-c781-43ae-a1f6-5c721f8a5697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 74, 88, ...,  0,  0,  0],\n",
       "       [ 1, 47, 80, ...,  0,  0,  0],\n",
       "       [ 1,  9, 88, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 1, 21, 39, ...,  0,  0,  0],\n",
       "       [ 1,  5, 86, ...,  0,  0,  0],\n",
       "       [ 1, 21, 88, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd768ff5-2158-4e46-9852-2551bf73d350",
   "metadata": {},
   "source": [
    "---\n",
    "# Flatten Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0ed294-df8f-47e7-b127-4d5d33a862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten_Data(x):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    x = x[x != 0]\n",
    "    x = x[x != 1]\n",
    "    x = x[x != 2]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7b115-1756-4eab-8494-a2c3cd6cfc77",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93d7530f-e127-4ab2-b006-d8f3711ca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_train = Flatten_Data(x_train)\n",
    "pre_y_flattened_train = Flatten_Data(pre_y_train)\n",
    "post_y_flattened_train = Flatten_Data(post_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f265779a-b996-46b6-b472-7847cdfcd4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom só pensa em ganh'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(pre_y_flattened_train[0:20], i_to_c_por)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f33669-803a-45b3-8856-9f01c2ec70d4",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f48728bc-d9fb-4f57-88b4-fac9bf7344b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_test = Flatten_Data(x_test)\n",
    "pre_y_flattened_test = Flatten_Data(pre_y_test)\n",
    "post_y_flattened_test = Flatten_Data(post_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293d03c-68bd-496d-b9fb-91ae84688fe0",
   "metadata": {},
   "source": [
    "--- \n",
    "# Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23e34f9-3066-4756-ad0b-ccc1a3194438",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 20\n",
    "block_size = 10\n",
    "seq_len = 58\n",
    "seq_len_padded = seq_len + 2\n",
    "maxlen = seq_len + 2 \n",
    "vocab_size = len(i_to_c_eng)\n",
    "num_chars_data_train = x_flattened_train.shape[0]\n",
    "num_chars_data_test = x_flattened_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e768970c-7af4-4626-9d6e-d712ab7de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if block_size-2 > seq_len:\n",
    "    raise ValueError(\"Block size should not be bigger than sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dce0777e-c8af-4835-833d-f1717dbf2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "91\n",
      "3118\n",
      "2813181\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(vocab_size)\n",
    "print(num_chars_data_train)\n",
    "print(num_chars_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56decdad-3c1d-496f-8f04-d783de100a1c",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc198d86-abfe-4a39-86fb-d8722200c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "def Partial_Batch(dataset, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    rints = rng.integers(low=0, high=num_chars-seq_len, size=1)[0]\n",
    "    \n",
    "    end_x = rints + seq_len\n",
    "    end_y = end_x + seq_len\n",
    "    \n",
    "    x = dataset[rints:end_x]\n",
    "    x = np.insert(x, 0, 1)\n",
    "    x = np.insert(x, x.shape[0], 2)\n",
    "    \n",
    "    pre_y = pre_y[end_x:end_y]\n",
    "    pre_y = np.insert(pre_y, 0, 1)\n",
    "    pre_y[-1] = 2\n",
    "    \n",
    "    post_y = post_y[end_x:end_y]\n",
    "    post_y = np.insert(post_y, post_y.shape[0], 2)\n",
    "    \n",
    "    batch = [x, pre_y, post_y]    \n",
    "    \n",
    "    batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=maxlen, padding='post', value=0)\n",
    "\n",
    "    padding = maxlen + (block_size-(maxlen%block_size))\n",
    "                        \n",
    "    if (batch.shape[1] % block_size) != 0:\n",
    "        batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=padding, padding='post', value=0)\n",
    "   \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ccc910a-fc5a-47c0-bf90-83587350fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_Batch(batch_size, x, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    x0 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        a, b, c, = Partial_Batch(x, pre_y, post_y, num_chars, seq_len, rng)\n",
    "    \n",
    "        x0.append(a)\n",
    "        y1.append(b)\n",
    "        y2.append(c) \n",
    "    \n",
    "    return np.asarray(x0), np.asarray(y1), np.asarray(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33326847-2235-4ec9-ae39-a808a34050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74123bfb-38c6-48cf-b830-39c7266398d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cd481db-7053-4ee9-98ca-b9e4f862b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t6kXó8Tâ\"86%ç(\"(QC%X\",6ã\"%óó8W\"6:r\"QótºT,6ã\"ã:W(çÂó\"(zXó%\"'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(batch_train[0][1], i_to_c_por)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befee659-3a43-4372-ae56-860a836497e1",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348583b3-a8cf-4d03-bc57-cbd8c106b890",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        \n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "        positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        attention_sum = content_attention + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        position = self._encoding_dense(relative_position_encoding)\n",
    "\n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "\n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.seq_len_padded = seq_len_padded\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "        self.output_layer = keras.layers.Dense((len(i_to_c_por)))\n",
    "    \n",
    "    def call(self, x, training=None):        \n",
    "        \n",
    "        model_output = []\n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.memory_length, self.embed_dim))\n",
    "        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        if self.memory_length > 0:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((mems[0], embeddings), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "            \n",
    "        for i in range(0, self.seq_len_padded, self.block_size):\n",
    "            block = embeddings[:,i:i+block_size]\n",
    "            rel_block = rel_embeddings[:,i:i+self.block_size+self.memory_length]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, relative_position_encoding=rel_block, state=mems)\n",
    "        \n",
    "            if i == 0:\n",
    "                model_output = output\n",
    "            else:\n",
    "                model_output = tf.concat([model_output, output], axis=1)\n",
    "        \n",
    "        model_output = self.output_layer(model_output)        \n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters \n",
    "embed_dim = 64\n",
    "num_layers = 8\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "memory_length = 20\n",
    "reuse_length = 0\n",
    "head_size = 32\n",
    "inner_size = 32\n",
    "dropout_rate = 0.01\n",
    "attention_dropout_rate = 0.01\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XlModel(block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "model.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726b5aa-bc90-44a8-bc2e-5bf755a0dcdc",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c9d81f1-57e9-416c-a7ea-bf8b8a3c2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(batches):\n",
    "    \n",
    "    batch_train, batch_test = batches\n",
    "    \n",
    "    x_train, pre_y_train, post_y_train = batch_train\n",
    "    x_test, pre_y_test, post_y_test = batch_test\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        output_train = model(x_train)\n",
    "        loss_train = MaskedSparseCategoricalCrossentropy(post_y_train, output_train)\n",
    "        \n",
    "    output_test = model(x_test)\n",
    "    loss_test = MaskedSparseCategoricalCrossentropy(post_y_test, output_test)\n",
    "        \n",
    "    accuracy_train = MaskedSparseCategoricalAccuracy(post_y_train, output_train)\n",
    "    accuracy_test = MaskedSparseCategoricalAccuracy(post_y_test, output_test)\n",
    "          \n",
    "    grads = tape.gradient(loss_train, model.trainable_weights)\n",
    "    \n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss_train, accuracy_train, loss_test, accuracy_test\n",
    "\n",
    "#@tf.function()\n",
    "# def dist_train_step(batches):\n",
    "#     loss_train, accuracy_train, loss_test, accuracy_test = strategy.run(train_step, args=(batches,))\n",
    "#     return strategy.reduce(tf.distribute.ReduceOp.SUM, loss_train, axis=None), accuracy_train, loss_test, accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c3a2e-02b5-4dbe-9527-a33c23338ce5",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_loss_train = []\n",
    "history_accuracy_train = []\n",
    "history_loss_test = []\n",
    "history_accuracy_test = []\n",
    "\n",
    "epochs = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/800 Train Loss: 1.2819429636001587 Train Accuracy = 0.6372881531715393 Test Loss: 4.510532379150391 Test Accuracy = 0.0932203382253646995"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_test \u001b[38;5;241m=\u001b[39m Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)\n\u001b[1;32m      6\u001b[0m batches \u001b[38;5;241m=\u001b[39m [batch_train, batch_test]\n\u001b[0;32m----> 8\u001b[0m loss_train, accuracy_train, loss_test, accuracy_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m history_loss_train\u001b[38;5;241m.\u001b[39mappend(loss_train)\n\u001b[1;32m     11\u001b[0m history_accuracy_train\u001b[38;5;241m.\u001b[39mappend(accuracy_train) \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "    batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)\n",
    "\n",
    "    batches = [batch_train, batch_test]\n",
    "\n",
    "    loss_train, accuracy_train, loss_test, accuracy_test = train_step(batches)\n",
    "\n",
    "    history_loss_train.append(loss_train)\n",
    "    history_accuracy_train.append(accuracy_train) \n",
    "    history_loss_test.append(loss_test)\n",
    "    history_accuracy_test.append(accuracy_test) \n",
    "\n",
    "    print(f\"\\r{epoch+1}/{epochs} Train Loss: {loss_train} Train Accuracy = {accuracy_train} Test Loss: {loss_test} Test Accuracy = {accuracy_test}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029869f-09b0-4ad7-8b69-06a155104667",
   "metadata": {},
   "source": [
    "---\n",
    "# Train vs Test Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c3ecbaa-4117-47fa-9ad3-de755a5b98e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_train)\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_test)\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_train)\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_test)\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031720c2-16f7-486c-83e0-d52cd42661ac",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "724ebe19-73b6-4aeb-b506-f24d6722ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = []\n",
    "final_accuracies_train = []\n",
    "final_accuracies_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc71d008-1f06-4d2b-a8b4-0ed7fb8603d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = np.loadtxt(\"/home/jovyan/TransformerXL/XL/Mem_Lens.txt\", dtype=np.float64)\n",
    "final_accuracies_train =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Train.txt\", dtype=np.float64)\n",
    "final_accuracies_test =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Test.txt\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b5293b4-e6a7-4fbb-a40a-512e965573c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.  40.  40. 100.  40.  40. 100. 100.   0. 100. 200. 200. 100. 100.\n",
      " 100.  20.]\n",
      "[0.927778   1.         1.         1.         1.         0.948756\n",
      " 0.996197   0.996197   0.998736   0.171383   0.994533   0.993177\n",
      " 0.988822   0.871429   0.989277   0.94491524]\n",
      "[0.188889   0.091156   0.091156   0.08429    0.108081   0.069364\n",
      " 0.076513   0.076513   0.072531   0.191552   0.067179   0.094536\n",
      " 0.065421   0.064865   0.079312   0.09237288]\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.append(mem_lens, memory_length)\n",
    "final_accuracies_train = np.append(final_accuracies_train, accuracy_train)\n",
    "final_accuracies_test = np.append(final_accuracies_test, accuracy_test)\n",
    "print(mem_lens)\n",
    "print(final_accuracies_train)\n",
    "print(final_accuracies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "779262e0-716e-431b-9e95-464bb76b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_accuracy_train = ({mem_len:accuracy_train for mem_len, accuracy_train in zip(mem_lens, final_accuracies_train)})\n",
    "mem_accuracy_test = ({mem_len:accuracy_test for mem_len, accuracy_test in zip(mem_lens, final_accuracies_test)})\n",
    "\n",
    "mem_lens_train_sorted = np.array([l for l in sorted(mem_accuracy_train)])\n",
    "mem_lens_test_sorted = np.array([l for l in sorted(mem_accuracy_test)])\n",
    "\n",
    "final_accuracies_train_sorted = np.array([mem_accuracy_train[l] for l in mem_lens_train_sorted])\n",
    "final_accuracies_test_sorted = np.array([mem_accuracy_test[l] for l in mem_lens_test_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9358244f-1349-4be4-bc01-cf7298f784d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/jovyan/TransformerXL/XL/Mem_Lens.txt', (mem_lens), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Train.txt', (final_accuracies_train), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Test.txt', (final_accuracies_test), fmt='%f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e931bbd-a1a9-4098-a4f2-0ef29f7fd89c",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91287ca0-3ff6-4d16-a8fe-837dcd5bf883",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e20aaf95-5aa2-4a44-8373-6eb89dc1fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, pre_y_train, post_y_train = batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "x_test, pre_y_test, post_y_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfa25c49-7ecd-439a-8069-dd08533a5224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the middle of the night.Several buildings were on fire.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(x_train[1], i_to_c_eng)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7976a654-ceda-4ff6-bba9-ac7df2aa6845",
   "metadata": {},
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0ea41ea-f3c5-46f1-b188-2e8708c6c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bf27abd-667b-4e4a-9909-421e92b4e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy[:,50:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9cdb7e65-0255-4d6d-9784-1d70199b4c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onNatal.Por que as aessoas se matam?A senhora podo....rrrr m'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = model.predict(copy)\n",
    "u = tf.argmax(u, -1)\n",
    "decode_seq(u[0], i_to_c_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8efcd49-a96f-46f5-b595-14d383c8ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assume that you're talking about Tom.Tom came back just in\n",
      "\n",
      "\n",
      "onNatal.Por que as aessoas se matam?A senhora podo comer q\n",
      "\n",
      "\n",
      "o Natal.Por que as pessoas se matam?A senhora pode comer q\n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[0], i_to_c_por)\n",
    "target = decode_seq(post_y_train[0], i_to_c_por)\n",
    "print(decode_seq(x_train[0], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26307e8f-a4f8-4c9a-b582-9af61d167f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the middle of the night.Several buildings were on fire.\n",
      "\n",
      "\n",
      "selevem oeamas.Um eivelope e  m selo, por favor.Tom não é \n",
      "\n",
      "\n",
      "avam em chamas.Um envelope e um selo, por favor.Tom não é \n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[1], i_to_c_por)\n",
    "target = decode_seq(post_y_train[1], i_to_c_por)\n",
    "print(decode_seq(x_train[1], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c395c788-2a6e-416a-a47a-db6752931963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister, my family doesn't watch TV.He went to the store at\n",
      "\n",
      "\n",
      " TV.Ele foi para a loja ne último minuto, justamente antes\n",
      "\n",
      "\n",
      " TV.Ele foi para a loja no último minuto, justamente antes\n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[2], i_to_c_por)\n",
    "target = decode_seq(post_y_train[2], i_to_c_por)\n",
    "print(decode_seq(x_train[2], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2605f273-e5ef-40b9-9552-5a86226e1ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s friends warned her that Tom was only after her money.The\n",
      "\n",
      "\n",
      " uam de uno Tom só  stava atrás de seu dinheiro.Oiúnico pr\n",
      "\n",
      "\n",
      "iram de que Tom só estava atrás de seu dinheiro.O único pr\n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[3], i_to_c_por)\n",
    "target = decode_seq(post_y_train[3], i_to_c_por)\n",
    "print(decode_seq(x_train[3], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b427d46e-597f-40f8-9fff-4fb46a5e16e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 60)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1ea5909-94e5-4b2b-bb58-47cdef22ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ooressisoi.m f mieOouse r nã aro a.gse rtanhoeno ccO  coe\n",
      "\n",
      "\n",
      "rabalhamos na mesma empresa.Eu quero que você volte para B\n"
     ]
    }
   ],
   "source": [
    "test = model.predict(x_test)\n",
    "test = tf.argmax(test, -1)\n",
    "test = decode_seq(test[0], i_to_c_por)\n",
    "target = decode_seq(post_y_test[0], i_to_c_por)\n",
    "print(test)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89ed4c-aeb3-4233-9f61-0ce36ff006f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
