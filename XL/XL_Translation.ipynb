{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccea5cd-89de-452b-afa6-703df9e9a604",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c14f8-8dd8-459f-81fe-a6708e6a8355",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {},
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb61447-8228-4b92-8f3f-0eddb8e33574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170304, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "data = []\n",
    "my_url = \"https://raw.githubusercontent.com/luisroque/deep-learning-articles/main/data/eng-por.txt\"\n",
    "with urllib.request.urlopen(my_url) as raw_data:\n",
    "    for line in raw_data:\n",
    "        data.append(line.decode(\"utf-8\").split('\\t')[0:2])\n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f65884-85ad-4ebd-ba51-29430504b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 170304//2\n",
    "split_point = 80\n",
    "data = data[100000:n_seq*2]\n",
    "np.random.shuffle(data) \n",
    "max_length = np.max([len(i) for i in data.flatten()]) + 2 \n",
    "max_length\n",
    "np.min([len(i) for i in data.flatten()]) + 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bbd23-6cbf-472f-bd6f-dc4a008b9072",
   "metadata": {},
   "source": [
    "---\n",
    "# Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c4c6ef-93df-4294-af8f-2fbaf01ca9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "    [mapping[i] for i in list(x)] + \\\n",
    "    [mapping['<STOP>']] + \\\n",
    "    [0]*(max_length-len(list(x))-2)\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b7c608-fe15-4420-a4fb-8356ebdcefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_eng = ['','<START>','<STOP>'] + list({char for word in data[:,0] for char in word})\n",
    "c_to_i_eng = {i_to_c_eng[i]:i for i in range(len(i_to_c_eng))}\n",
    "i_to_c_eng[1] = i_to_c_eng[2] = ''\n",
    "i_to_c_por = ['','<START>','<STOP>'] + list({char for word in data[:,1] for char in word})\n",
    "c_to_i_por = {i_to_c_por[i]:i for i in range(len(i_to_c_por))}\n",
    "i_to_c_por[1] = i_to_c_por[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcaf882a-b84a-49c5-8326-77f91be9d319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i_to_c_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6185f0ea-f949-427d-a6e4-d3c0ab45f960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i_to_c_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6889059-fdd9-43e2-b0bc-199ff07c4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 186)\n",
      "(80, 185)\n",
      "(80, 185)\n",
      "(70224, 186)\n",
      "(70224, 185)\n",
      "(70224, 185)\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_eng,max_length) for x in data[:,0]])\n",
    "Y = np.vstack([encode_seq(x,c_to_i_por,max_length) for x in data[:,1]])\n",
    "x_train = X[:split_point]\n",
    "x_test = X[split_point:]\n",
    "pre_y_train = Y[:,0:-1][:split_point]\n",
    "pre_y_test = Y[:,0:-1][split_point:]\n",
    "post_y_train = Y[:,1:][:split_point]\n",
    "post_y_test = Y[:,1:][split_point:]\n",
    "print(x_train.shape)\n",
    "print(pre_y_train.shape)\n",
    "print(post_y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(pre_y_test.shape)\n",
    "print(post_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1083fa6e-f561-4cad-8107-3b935126a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Não deixe isso acontecer outra vez.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(post_y_train[0], i_to_c_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca664bb9-544b-4264-8da4-34acaca2d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 186)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f725c81-51b2-4497-981a-a15981f9dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70224, 186)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6a03188-6146-4487-8483-b0a3296d4426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  40,  85,  35,  71,  63,  91,  16,  50,  91,  71,  16,  60,\n",
       "        60,  35,  71,  66,  72,  35,  25, 108,  91,  72,  91,  55,  71,\n",
       "        35,  11, 108,  55,  66,  71,  78,  91, 105, 107,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154bc9ef-0e64-4ad5-96d4-13012994718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40,  85,  35,  71,  63,  91,  16,  50,  91,  71,  16,  60,  60,\n",
       "        35,  71,  66,  72,  35,  25, 108,  91,  72,  91,  55,  71,  35,\n",
       "        11, 108,  55,  66,  71,  78,  91, 105, 107,   2,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e33b84-c781-43ae-a1f6-5c721f8a5697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4, 32, ...,  0,  0,  0],\n",
       "       [ 1, 10, 69, ...,  0,  0,  0],\n",
       "       [ 1, 74, 32, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 1, 50, 32, ...,  0,  0,  0],\n",
       "       [ 1, 74, 32, ...,  0,  0,  0],\n",
       "       [ 1, 74, 69, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd768ff5-2158-4e46-9852-2551bf73d350",
   "metadata": {},
   "source": [
    "---\n",
    "# Flatten Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0ed294-df8f-47e7-b127-4d5d33a862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten_Data(x):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    x = x[x != 0]\n",
    "    x = x[x != 1]\n",
    "    x = x[x != 2]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7b115-1756-4eab-8494-a2c3cd6cfc77",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93d7530f-e127-4ab2-b006-d8f3711ca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_train = Flatten_Data(x_train)\n",
    "pre_y_flattened_train = Flatten_Data(pre_y_train)\n",
    "post_y_flattened_train = Flatten_Data(post_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f265779a-b996-46b6-b472-7847cdfcd4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Não deixe isso acont'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(pre_y_flattened_train[0:20], i_to_c_por)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f33669-803a-45b3-8856-9f01c2ec70d4",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f48728bc-d9fb-4f57-88b4-fac9bf7344b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_test = Flatten_Data(x_test)\n",
    "pre_y_flattened_test = Flatten_Data(pre_y_test)\n",
    "post_y_flattened_test = Flatten_Data(post_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293d03c-68bd-496d-b9fb-91ae84688fe0",
   "metadata": {},
   "source": [
    "--- \n",
    "# Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23e34f9-3066-4756-ad0b-ccc1a3194438",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 20\n",
    "block_size = 10\n",
    "seq_len = 58\n",
    "seq_len_padded = seq_len + 2\n",
    "maxlen = seq_len + 2 \n",
    "vocab_size = len(i_to_c_eng)\n",
    "num_chars_data_train = x_flattened_train.shape[0]\n",
    "num_chars_data_test = x_flattened_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e768970c-7af4-4626-9d6e-d712ab7de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if block_size-2 > seq_len:\n",
    "    raise ValueError(\"Block size should not be bigger than sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dce0777e-c8af-4835-833d-f1717dbf2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "91\n",
      "3207\n",
      "2813092\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(vocab_size)\n",
    "print(num_chars_data_train)\n",
    "print(num_chars_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56decdad-3c1d-496f-8f04-d783de100a1c",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc198d86-abfe-4a39-86fb-d8722200c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "def Partial_Batch(dataset, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    rints = rng.integers(low=0, high=num_chars-seq_len, size=1)[0]\n",
    "    \n",
    "    end_x = rints + seq_len\n",
    "    end_y = end_x + seq_len\n",
    "    \n",
    "    x = dataset[rints:end_x]\n",
    "    x = np.insert(x, 0, 1)\n",
    "    x = np.insert(x, x.shape[0], 2)\n",
    "    \n",
    "    pre_y = pre_y[end_x:end_y]\n",
    "    pre_y = np.insert(pre_y, 0, 1)\n",
    "    pre_y[-1] = 2\n",
    "    \n",
    "    post_y = post_y[end_x:end_y]\n",
    "    post_y = np.insert(post_y, post_y.shape[0], 2)\n",
    "    \n",
    "    batch = [x, pre_y, post_y]    \n",
    "    \n",
    "    batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=maxlen, padding='post', value=0)\n",
    "\n",
    "    padding = maxlen + (block_size-(maxlen%block_size))\n",
    "                        \n",
    "    if (batch.shape[1] % block_size) != 0:\n",
    "        batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=padding, padding='post', value=0)\n",
    "   \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ccc910a-fc5a-47c0-bf90-83587350fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_Batch(batch_size, x, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    x0 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        a, b, c, = Partial_Batch(x, pre_y, post_y, num_chars, seq_len, rng)\n",
    "    \n",
    "        x0.append(a)\n",
    "        y1.append(b)\n",
    "        y2.append(c) \n",
    "    \n",
    "    return np.asarray(x0), np.asarray(y1), np.asarray(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33326847-2235-4ec9-ae39-a808a34050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74123bfb-38c6-48cf-b830-39c7266398d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cd481db-7053-4ee9-98ca-b9e4f862b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p4ê\\u200bWpRQ4ºQ\"Tsrp4bksk9W4jsk9BksàpNsj4p%gsk9BksTbNsBs\\u200bQCQkB'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(batch_train[0][1], i_to_c_por)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befee659-3a43-4372-ae56-860a836497e1",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348583b3-a8cf-4d03-bc57-cbd8c106b890",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        \n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "        positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        attention_sum = content_attention + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        position = self._encoding_dense(relative_position_encoding)\n",
    "\n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            #new_mems.append(_cache_memory(content_stream, state[i], self._memory_length, self._reuse_length))\n",
    "\n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "            \n",
    "            new_mems.append(_cache_memory(content_stream, state[i], self._memory_length, self._reuse_length))\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.seq_len_padded = seq_len_padded\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "        self.output_layer = keras.layers.Dense((len(i_to_c_por)))\n",
    "    \n",
    "    def call(self, x, training=None):        \n",
    "        \n",
    "        model_output = []\n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.memory_length, self.embed_dim))\n",
    "        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        if self.memory_length > 0:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((mems[0], embeddings), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "            \n",
    "        for i in range(0, self.seq_len_padded, self.block_size):\n",
    "            block = embeddings[:,i:i+block_size]\n",
    "            rel_block = rel_embeddings[:,i:i+self.block_size+self.memory_length]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, relative_position_encoding=rel_block, state=mems)\n",
    "        \n",
    "            if i == 0:\n",
    "                model_output = output\n",
    "            else:\n",
    "                model_output = tf.concat([model_output, output], axis=1)\n",
    "        \n",
    "        model_output = self.output_layer(model_output)        \n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters \n",
    "embed_dim = 64\n",
    "num_layers = 8\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "memory_length = 20\n",
    "reuse_length = 0\n",
    "head_size = 32\n",
    "inner_size = 32\n",
    "dropout_rate = 0.01\n",
    "attention_dropout_rate = 0.01\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XlModel(block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "model.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726b5aa-bc90-44a8-bc2e-5bf755a0dcdc",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c9d81f1-57e9-416c-a7ea-bf8b8a3c2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(batches):\n",
    "    \n",
    "    batch_train, batch_test = batches\n",
    "    \n",
    "    x_train, pre_y_train, post_y_train = batch_train\n",
    "    x_test, pre_y_test, post_y_test = batch_test\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        output_train = model(x_train)\n",
    "        loss_train = MaskedSparseCategoricalCrossentropy(post_y_train, output_train)\n",
    "        \n",
    "    output_test = model(x_test)\n",
    "    loss_test = MaskedSparseCategoricalCrossentropy(post_y_test, output_test)\n",
    "        \n",
    "    accuracy_train = MaskedSparseCategoricalAccuracy(post_y_train, output_train)\n",
    "    accuracy_test = MaskedSparseCategoricalAccuracy(post_y_test, output_test)\n",
    "          \n",
    "    grads = tape.gradient(loss_train, model.trainable_weights)\n",
    "    \n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss_train, accuracy_train, loss_test, accuracy_test\n",
    "\n",
    "#@tf.function()\n",
    "# def dist_train_step(batches):\n",
    "#     loss_train, accuracy_train, loss_test, accuracy_test = strategy.run(train_step, args=(batches,))\n",
    "#     return strategy.reduce(tf.distribute.ReduceOp.SUM, loss_train, axis=None), accuracy_train, loss_test, accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c3a2e-02b5-4dbe-9527-a33c23338ce5",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_loss_train = []\n",
    "history_accuracy_train = []\n",
    "history_loss_test = []\n",
    "history_accuracy_test = []\n",
    "\n",
    "epochs = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 Train Loss: 0.06863654404878616 Train Accuracy = 0.9796609878540039 Test Loss: 8.349418640136719 Test Accuracy = 0.087288133800029753"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "    batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)\n",
    "\n",
    "    batches = [batch_train, batch_test]\n",
    "\n",
    "    loss_train, accuracy_train, loss_test, accuracy_test = train_step(batches)\n",
    "\n",
    "    history_loss_train.append(loss_train)\n",
    "    history_accuracy_train.append(accuracy_train) \n",
    "    history_loss_test.append(loss_test)\n",
    "    history_accuracy_test.append(accuracy_test) \n",
    "\n",
    "    print(f\"\\r{epoch+1}/{epochs} Train Loss: {loss_train} Train Accuracy = {accuracy_train} Test Loss: {loss_test} Test Accuracy = {accuracy_test}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029869f-09b0-4ad7-8b69-06a155104667",
   "metadata": {},
   "source": [
    "---\n",
    "# Train vs Test Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c3ecbaa-4117-47fa-9ad3-de755a5b98e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_train)\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_test)\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_train)\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_test)\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031720c2-16f7-486c-83e0-d52cd42661ac",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "724ebe19-73b6-4aeb-b506-f24d6722ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = []\n",
    "final_accuracies_train = []\n",
    "final_accuracies_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc71d008-1f06-4d2b-a8b4-0ed7fb8603d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = np.loadtxt(\"/home/jovyan/TransformerXL/XL/Mem_Lens.txt\", dtype=np.float64)\n",
    "final_accuracies_train =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Train.txt\", dtype=np.float64)\n",
    "final_accuracies_test =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Test.txt\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b5293b4-e6a7-4fbb-a40a-512e965573c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.  40.  40. 100.  40.  40. 100. 100.   0. 100. 200. 200. 100. 100.\n",
      " 100.  20.]\n",
      "[0.927778   1.         1.         1.         1.         0.948756\n",
      " 0.996197   0.996197   0.998736   0.171383   0.994533   0.993177\n",
      " 0.988822   0.871429   0.989277   0.94491524]\n",
      "[0.188889   0.091156   0.091156   0.08429    0.108081   0.069364\n",
      " 0.076513   0.076513   0.072531   0.191552   0.067179   0.094536\n",
      " 0.065421   0.064865   0.079312   0.09237288]\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.append(mem_lens, memory_length)\n",
    "final_accuracies_train = np.append(final_accuracies_train, accuracy_train)\n",
    "final_accuracies_test = np.append(final_accuracies_test, accuracy_test)\n",
    "print(mem_lens)\n",
    "print(final_accuracies_train)\n",
    "print(final_accuracies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "779262e0-716e-431b-9e95-464bb76b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_accuracy_train = ({mem_len:accuracy_train for mem_len, accuracy_train in zip(mem_lens, final_accuracies_train)})\n",
    "mem_accuracy_test = ({mem_len:accuracy_test for mem_len, accuracy_test in zip(mem_lens, final_accuracies_test)})\n",
    "\n",
    "mem_lens_train_sorted = np.array([l for l in sorted(mem_accuracy_train)])\n",
    "mem_lens_test_sorted = np.array([l for l in sorted(mem_accuracy_test)])\n",
    "\n",
    "final_accuracies_train_sorted = np.array([mem_accuracy_train[l] for l in mem_lens_train_sorted])\n",
    "final_accuracies_test_sorted = np.array([mem_accuracy_test[l] for l in mem_lens_test_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9358244f-1349-4be4-bc01-cf7298f784d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/jovyan/TransformerXL/XL/Mem_Lens.txt', (mem_lens), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Train.txt', (final_accuracies_train), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Test.txt', (final_accuracies_test), fmt='%f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e931bbd-a1a9-4098-a4f2-0ef29f7fd89c",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91287ca0-3ff6-4d16-a8fe-837dcd5bf883",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e20aaf95-5aa2-4a44-8373-6eb89dc1fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, pre_y_train, post_y_train = batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "x_test, pre_y_test, post_y_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfa25c49-7ecd-439a-8069-dd08533a5224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hree hours ago.When is the ship due to arrive?He was very '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(x_train[1], i_to_c_eng)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7976a654-ceda-4ff6-bba9-ac7df2aa6845",
   "metadata": {},
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0ea41ea-f3c5-46f1-b188-2e8708c6c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1bf27abd-667b-4e4a-9909-421e92b4e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy[:,50:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cdb7e65-0255-4d6d-9784-1d70199b4c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ados EiA é muito mass avançada.Com quem você queru   ff'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = model.predict(copy)\n",
    "u = tf.argmax(u, -1)\n",
    "decode_seq(u[0], i_to_c_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8efcd49-a96f-46f5-b595-14d383c8ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt to go to Australia with?This is mine, and this is yours\n",
      "\n",
      "\n",
      "ados EiA é muito mass avançada.Com quem você queruis para \n",
      "\n",
      "\n",
      " dos EUA é muito mais avançada.Com quem você quer ir para \n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[0], i_to_c_por)\n",
    "target = decode_seq(post_y_train[0], i_to_c_por)\n",
    "print(decode_seq(x_train[0], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26307e8f-a4f8-4c9a-b582-9af61d167f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hree hours ago.When is the ship due to arrive?He was very \n",
      "\n",
      "\n",
      "sahoras adr s.Quando é para ornavio chegar?Elexera muito s\n",
      "\n",
      "\n",
      "s horas atrás.Quando é para o navio chegar?Ele era muito s\n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[1], i_to_c_por)\n",
    "target = decode_seq(post_y_train[1], i_to_c_por)\n",
    "print(decode_seq(x_train[1], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c395c788-2a6e-416a-a47a-db6752931963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o's older, your mother or your father?We have room for thi\n",
      "\n",
      "\n",
      "ocutido irso comigo.Tom tomou Mary em seus braçosse a beij\n",
      "\n",
      "\n",
      "scutido isso comigo.Tom tomou Mary em seus braços e a beij\n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[2], i_to_c_por)\n",
    "target = decode_seq(post_y_train[2], i_to_c_por)\n",
    "print(decode_seq(x_train[2], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2605f273-e5ef-40b9-9552-5a86226e1ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t hand.I've decided to let Tom do that for me.Tom added Ma\n",
      "\n",
      "\n",
      "m adiEionou o nome de Maria à lista.Eu rostaria de ir, mas\n",
      "\n",
      "\n",
      "m adicionou o nome de Maria à lista.Eu gostaria de ir, mas\n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[3], i_to_c_por)\n",
    "target = decode_seq(post_y_train[3], i_to_c_por)\n",
    "print(decode_seq(x_train[3], i_to_c_eng))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b427d46e-597f-40f8-9fff-4fb46a5e16e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 60)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1ea5909-94e5-4b2b-bb58-47cdef22ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". violêoqia.Eu acho suene To o yostetoteariir o ieqor mso \n",
      "\n",
      "\n",
      "Como você pode pensar em uma coisa dessas?Adoraria saber t\n"
     ]
    }
   ],
   "source": [
    "test = model.predict(x_test)\n",
    "test = tf.argmax(test, -1)\n",
    "test = decode_seq(test[0], i_to_c_por)\n",
    "target = decode_seq(post_y_test[0], i_to_c_por)\n",
    "print(test)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89ed4c-aeb3-4233-9f61-0ce36ff006f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
